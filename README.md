# Awesome-LLM-based-Evaluators

In the realm of evaluating large language models, automated LLM-based evaluations have emerged as a scalable and efficient alternative to human evaluation. This repo includes papers about the LLM-based evaluators.

## LLM-based evaluators

| Title                                                        | Venue         | Year | Authors                                                      | Citation Count | Code                                                         |
| ------------------------------------------------------------ | ------------- | ---- | ------------------------------------------------------------ | -------------- | ------------------------------------------------------------ |
| **[Chateval: Towards better llm-based evaluators through multi-agent debate](https://arxiv.org/abs/2308.07201)** | ICLR          | 2024 | Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu | 87             | [GitHub](https://github.com/thunlp/ChatEval)                 |
| **[Can large language models be an alternative to human evaluations?](https://arxiv.org/abs/2305.01937)** | ACL           | 2023 | Cheng-Han Chiang, Hung-yi Lee                                | 133            | -                                                            |
| **[Are large language model-based evaluators the solution to scaling up multilingual evaluation?](https://arxiv.org/abs/2309.07462)** | EACL Findings | 2024 | Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram | 12             | [GitHub](https://github.com/hadarishav/LLM-Eval)             |
| **[Judging llm-as-a-judge with mt-bench and chatbot arena](https://arxiv.org/abs/2306.05685)** | NeurIPS       | 2024 | Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica | 510            | [GitHub](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge) |
| **[Dyval: Graph-informed dynamic evaluation of large language models](https://arxiv.org/abs/2309.17167)** | ICLR          | 2024 | Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie | 6              | [GitHub](https://github.com/microsoft/promptbench)           |
| **[PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization](https://openreview.net/pdf?id=5Nn2BLV7SB)** | ICLR          | 2024 | Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Wenjin Yao, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang | 52             | [GitHub](https://github.com/WeOpenML/PandaLM)                |
| **[LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models](https://arxiv.org/abs/2305.13711)** | NLP4ConvAI    | 2023 | Yen-Ting Lin, Yun-Nung Chen                                  | 42             | -                                                            |
