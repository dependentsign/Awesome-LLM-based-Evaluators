# Awesome-LLM-based-Evaluators

In the realm of evaluating large language models, automated LLM-based evaluations have emerged as a scalable and efficient alternative to human evaluation. This repo includes papers about the LLM-based evaluators.

## LLM-based evaluators

| Title & Authors                                              | Venue         | Year | Citation Count | Code                                                         |
| ------------------------------------------------------------ | ------------- | ---- | -------------- | ------------------------------------------------------------ |
| [**Chateval: Towards better llm-based evaluators through multi-agent debate**](https://arxiv.org/abs/2308.07201) by Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu | ICLR          | 2024 | 87             | [GitHub](https://github.com/thunlp/ChatEval)                 |
| [**Dyval: Graph-informed dynamic evaluation of large language models**](https://arxiv.org/abs/2309.17167) by Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie | ICLR          | 2024 | 6              | [GitHub](https://github.com/microsoft/promptbench)           |
| [**PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization**](https://openreview.net/pdf?id=5Nn2BLV7SB) by Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Wenjin Yao, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang | ICLR          | 2024 | 52             | [GitHub](https://github.com/WeOpenML/PandaLM)                |
| [**Can large language models be an alternative to human evaluations?**](https://arxiv.org/abs/2305.01937) by Cheng-Han Chiang, Hung-yi Lee | ACL           | 2023 | 133            | -                                                            |
| [**LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models**](https://arxiv.org/abs/2305.13711) by Yen-Ting Lin, Yun-Nung Chen | NLP4ConvAI    | 2023 | 42             | -                                                            |
| [**Are large language model-based evaluators the solution to scaling up multilingual evaluation?**](https://arxiv.org/abs/2309.07462) by Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram | EACL Findings | 2024 | 12             | [GitHub](https://github.com/hadarishav/LLM-Eval)             |
| [**Judging llm-as-a-judge with mt-bench and chatbot arena**](https://arxiv.org/abs/2306.05685) by Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica | NeurIPS       | 2023 | 510            | [GitHub](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge) |
| [**Calibrating LLM-Based Evaluator**](https://arxiv.org/abs/2309.13308) by Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang | arXiv         | 2023 | 7              | -                                                            |
| [**LLM-based NLG Evaluation: Current Status and Challenges**](https://arxiv.org/abs/2402.01383) by Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan | arXiv         | 2024 | 1              | -                                                            |
| [**Are LLM-based Evaluators Confusing NLG Quality Criteria?**](https://arxiv.org/abs/2402.12055) by Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan | arXiv         | 2024 | -              | -                                                            |
| [**PRE: A Peer Review Based Large Language Model Evaluator**](https://arxiv.org/abs/2401.15641) by Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu | arXiv         | 2024 | 1              | [GitHub](https://github.com/chuzhumin98/PRE)                 |
| [**Allure: A systematic protocol for auditing and improving llm-based evaluation of text using iterative in-context-learning**](https://arxiv.org/abs/2309.13701) by Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, Ida Momennejad | arXiv         | 2023 | 4              | -                                                            |
| [**Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate**](https://arxiv.org/abs/2401.16788) by Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu | arXiv         | 2024 | -              | [GitHub](https://github.com/GAIR-NLP/scaleeval)              |
| [**Split and merge: Aligning position biases in large language model based evaluators**](https://arxiv.org/abs/2310.01432) by Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu | arXiv         | 2023 | 8              |                                                              |
| [**One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation****](https://arxiv.org/abs/2402.11683) by Tejpalsingh Siledar, Swaroop Nath, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Swaprava Nath, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera | arXiv         | 2024 | -              | [GitHub](https://github.com/tjsiledar/SummEval-OP)           |
| [**Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation**](https://arxiv.org/abs/2311.18702)by Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang | arXiv         | 2023 | 9              | [GitHub](https://github.com/thu-coai/CritiqueLLM)            |
