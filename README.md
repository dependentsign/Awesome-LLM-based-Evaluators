# Awesome-LLM-based-Evaluators

In the realm of evaluating large language models, automated LLM-based evaluations have emerged as a scalable and efficient alternative to human evaluation. This repo includes papers about the LLM-based evaluators.

## LLM-based evaluators

| Title & Authors                                              | Venue         | Year | Citation Count | Code                                                         |
| ------------------------------------------------------------ | ------------- | ---- | -------------- | ------------------------------------------------------------ |
| [**Can large language models be an alternative to human evaluations?**](https://arxiv.org/abs/2305.01937) by Cheng-Han Chiang, Hung-yi Lee | ACL           | 2023 | 133            | -                                                            |
| [**LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models**](https://arxiv.org/abs/2305.13711) by Yen-Ting Lin, Yun-Nung Chen | NLP4ConvAI    | 2023 | 42             | -                                                            |
| [**Are large language model-based evaluators the solution to scaling up multilingual evaluation?**](https://arxiv.org/abs/2309.07462) by Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram | EACL Findings | 2024 | 12             | [GitHub](https://github.com/hadarishav/LLM-Eval)             |
| [**Judging llm-as-a-judge with mt-bench and chatbot arena**](https://arxiv.org/abs/2306.05685) by Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica | NeurIPS       | 2023 | 510            | [GitHub](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge) |
| [**Chateval: Towards better llm-based evaluators through multi-agent debate**](https://arxiv.org/abs/2308.07201) by Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu | ICLR          | 2024 | 87             | [GitHub](https://github.com/thunlp/ChatEval)                 |
| [**Dyval: Graph-informed dynamic evaluation of large language models**](https://arxiv.org/abs/2309.17167) by Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie | ICLR          | 2024 | 6              | [GitHub](https://github.com/microsoft/promptbench)           |
| [**PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization**](https://openreview.net/pdf?id=5Nn2BLV7SB) by Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Wenjin Yao, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang | ICLR          | 2024 | 52             | [GitHub](https://github.com/WeOpenML/PandaLM)                |
